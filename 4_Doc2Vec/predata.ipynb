{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/opt/gongxf/python3_pj/Robot/generate_data/15_16_17_18_question.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保留包含汉字字符\n",
    "def extract_chinese(doc):\n",
    "    pattern='[\\u4e00-\\u9fa5]+'\n",
    "    if re.compile(pattern).search(str(doc)):\n",
    "        return doc\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除http，&nbsp、link等\n",
    "def extract_span(doc):\n",
    "    pattern = ('\\[link.+?\\]+|\\[\\/link]|http+[^\\u4e00-\\u9fa5]+|<[^>]+>|&nbsp|[\\n]|Name+[^\\u4e00-\\u9fa5]object+')\n",
    "    return re.compile(pattern).sub('', str(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log(path):\n",
    "    df=pd.read_csv(path,header=None,names=['content'],engine='python',encoding='utf-8',error_bad_lines=False,)\n",
    "    dff=df['content'].apply(extract_chinese)\n",
    "    dff.to_csv(\"./data/Knowledge_0816.txt\", mode='a+',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################针对语料库问题数据进行处理############################\n",
    "# load_log(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############整个语料库数据处理#############################\n",
    "in_path = '/opt/gongxf/python3_pj/Robot/original_data/bz_nlp_15_16_17_18.csv'\n",
    "out_path='./data/bz_nlp_15_16_17_18_0816.txt'\n",
    "#语料库合并、合并客户会话，一个session合并成一个doc\n",
    "def Pretreatment_doc(in_path,out_path):\n",
    "    #包含中英文\n",
    "    df = pd.read_csv(in_path, sep='\\t', error_bad_lines=False)\n",
    "    dff=df['bz_tmp1.msg_content'].apply(extract_span)\n",
    "    dff=dff.apply(extract_chinese)\n",
    "    dff.to_csv(out_path, mode='a+',header=False, index=False)\n",
    "Pretreatment_doc(in_path,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.951 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.951 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import sys\n",
    "import gensim\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import jieba\n",
    "from numpy import linalg as la\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "jieba.load_userdict(\"/opt/gongxf/python3_pj/Robot/original_data/finWordDict.txt\")\n",
    "all_text_path='/DATA/1_DataCache/FinCorpus/all_data_pure.csv'\n",
    "text_path='/opt/gongxf/python3_pj/Robot/13_Doc2Vec/data/Knowledge_0816.txt'\n",
    "\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "def stop_words():\n",
    "    stop_words = []\n",
    "    with open('/opt/gongxf/python3_pj/Robot/original_data/stop_words.txt', \"r\", encoding=\"UTF-8\") as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            stop_words.append(line[:-1])\n",
    "            line = f.readline()\n",
    "    return stop_words\n",
    "#停用词字典列表\n",
    "stopwords_list=stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_cut_file():\n",
    "    file = open('./data/Knowledge_cutstop_0816.txt', 'w', encoding='utf-8')\n",
    "    all_doc_list=[]\n",
    "    i=0\n",
    "    # print('停用词读取完毕，共{n}个词'.format(n=len(stop_words)))\n",
    "    with open(text_path, \"r\", encoding='UTF-8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            while '\\n' in line:\n",
    "                line = line.replace('\\n', '')\n",
    "            while ' ' in line:\n",
    "                line = line.replace(' ', '')\n",
    "            if len(line) > 0:  # 如果句子非空\n",
    "                all_doc_list.append((i,line))\n",
    "                i+=1\n",
    "                raw_words_cut = list(jieba.cut(line, cut_all=False))\n",
    "                raw_words=[word for word in raw_words_cut if word not in stopwords_list]\n",
    "                if len(raw_words)>0:\n",
    "                    file.write(','.join(raw_words)+'\\n')\n",
    "            line = f.readline()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulid_cut_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Knowledge_cutstop_0816.txt\", 'r',encoding='utf-8') as cf:\n",
    "    docs = cf.readlines()\n",
    "x_train = []\n",
    "for i, text in enumerate(docs):\n",
    "    word_list = text.split(',')\n",
    "    word_list_len = len(word_list)\n",
    "    word_list[word_list_len - 1] = word_list[word_list_len - 1].strip()\n",
    "    document = TaggededDocument(word_list, tags=[str(i)])\n",
    "    x_train.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename=object):\n",
    "        self.filename =filename\n",
    "    def __iter__(self):\n",
    "        with open(self.filename,'r') as infile:\n",
    "            data=infile.readlines(); \n",
    "            print(\"length: \", len(data))   \n",
    "        for i,text in tqdm(enumerate(data)):\n",
    "            word_list=text.split(',')\n",
    "            word_list_len = len(word_list)\n",
    "            word_list[word_list_len - 1] = word_list[word_list_len - 1].strip()\n",
    "            yield LabeledSentence(words=word_list, tags=[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =LabeledLineSentence('./data/Knowledge_cutstop_0816.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/gongxf/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "0it [00:00, ?it/s]/opt/gongxf/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  if sys.path[0] == '':\n",
      "4895it [00:00, 48940.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [03:11, 101914.42it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [16:55, 19180.23it/s]\n",
      "197it [00:00, 295.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [16:58, 19136.77it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [16:52, 19232.20it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [17:04, 19012.84it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  19481530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19481530it [16:54, 19195.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dm = Doc2Vec(sentences, min_count=50, window=5, size=100, sample=1e-6, negative=5, dm=0,workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm.save('model/Knowledge_cbow_816')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
